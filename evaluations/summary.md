# Evaluation Summary

## Overview

[PLACEHOLDER: high-level summary of all evaluation results]

## Key Findings

### First fully correct pipeline per model family

| Model Family | First Fully Correct Version | Notes |
|:-------------|:----------------------------|:------|
| OpenAI | GPT-5 (high) | [PLACEHOLDER] |
| Claude | Opus 4.5 | [PLACEHOLDER] |
| Gemini | 3 Pro | [PLACEHOLDER] |

### Hardest step across all models

[PLACEHOLDER: which pipeline step had the lowest average score?]

### Most common failure mode per step

| Step | Most Common Failure | Models Affected |
|:-----|:-------------------|:----------------|
| 1. Basecalling | [PLACEHOLDER] | [PLACEHOLDER] |
| 2. Quality control | [PLACEHOLDER] | [PLACEHOLDER] |
| 3. Host depletion | [PLACEHOLDER] | [PLACEHOLDER] |
| 4. Taxonomic classification | [PLACEHOLDER] | [PLACEHOLDER] |
| 5. Assembly | [PLACEHOLDER] | [PLACEHOLDER] |
| 6. Binning | [PLACEHOLDER] | [PLACEHOLDER] |
| 7. Functional annotation | [PLACEHOLDER] | [PLACEHOLDER] |

### Error compounding

[PLACEHOLDER: how did errors at early steps affect later steps? Were there cases where an error at step N caused cascade failures at steps N+1 through 7?]

## Scoring Heatmap

![Scoring heatmap](../results/figures/scoring_heatmap.png)

[PLACEHOLDER: insert generated heatmap and discussion]

## Conclusions

[PLACEHOLDER: what do these results mean for scientists considering LLMs as pipeline development aids?]

## Recommendations

[PLACEHOLDER: practical guidance based on findings]
